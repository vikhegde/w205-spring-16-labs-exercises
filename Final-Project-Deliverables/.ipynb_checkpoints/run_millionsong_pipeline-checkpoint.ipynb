{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import hdf5_getters\n",
    "\n",
    "from ml_pipeline import *\n",
    "from model_common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for processing Million Song dataset follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsamples_list = [5, 10, 20, 50, 100, 250, 500, 1000, 5000, 10000, 20000, 30000, 40000,50000, 75000, 100000]\n",
    "\n",
    "class CollectH5Files(MLRaw):\n",
    "    def __init__(self):\n",
    "        super(CollectH5Files, self).__init__()\n",
    "        self.class_name = \"CollectH5Files\"\n",
    "        self.mime_type = 'application/x-hdf'\n",
    "        self.nsongs = None\n",
    "        self.max_process = None\n",
    "        self.h5_files = []\n",
    "        \n",
    "        input_data = {\"final_loc\" : \"\"}\n",
    "        input_json = json.dumps(input_data)\n",
    "        output_data = {\"h5_files\" : \"\"}\n",
    "        output_json = json.dumps(output_data)\n",
    "        \n",
    "        self.istr_jsons = input_json\n",
    "        self.ostr_jsons = output_json\n",
    "        \n",
    "    def do_walk(self, filepath, input_data):\n",
    "        \n",
    "        input_data[\"count\"] += 1\n",
    "        if get_file_type(filepath, mime=True) != self.mime_type:\n",
    "            return {\"stop\" : False}\n",
    "        \n",
    "        # Uncomment this if you want a subset\n",
    "        if len(self.h5_files) >= self.nsongs:\n",
    "            return {\"stop\" : True}\n",
    "        \n",
    "        if input_data[\"count\"] > self.max_process:\n",
    "            return {\"stop\" : True}\n",
    "        \n",
    "        h5 = hdf5_getters.open_h5_file_read(filepath)\n",
    "        song_year = int(hdf5_getters.get_year(h5).item())\n",
    "        h5.close()\n",
    "        \n",
    "        if song_year is None or song_year == '' or song_year == 0 or song_year < 1800 or song_year > 2100:\n",
    "            return {\"stop\" : False}\n",
    "        \n",
    "        self.h5_files.append(filepath)\n",
    "        \n",
    "        return {\"stop\" : False}\n",
    "            \n",
    "    def do_run(self, input_data, traversal):\n",
    "        if traversal == \"POST\":\n",
    "            if not \"h5_files\" in input_data.keys():\n",
    "                input_data[\"h5_files\"] = \"\"\n",
    "            return input_data\n",
    "            \n",
    "        self.final_loc = input_data['final_loc']\n",
    "        \n",
    "        input_data[\"count\"] = 0\n",
    "        self.walk_files(self.final_loc, input_data)\n",
    "        del input_data[\"count\"]\n",
    "        \n",
    "        input_data[\"final_loc\"] = self.final_loc\n",
    "        input_data['h5_files'] = self.h5_files\n",
    "        \n",
    "        return input_data\n",
    "        \n",
    "class ExtractTrackData(MLDerive):\n",
    "    def __init__(self):\n",
    "        super(ExtractTrackData, self).__init__()\n",
    "        self.class_name = \"ExtractTrackData\"\n",
    "        self.mime_type = \"application/x-hdf\"\n",
    "        self.h5_files = None\n",
    "        self.nsongs = None\n",
    "        self.train_frac = None\n",
    "        \n",
    "        input_data = {\"final_loc\" : \"\", \"h5_files\" : \"\"}\n",
    "        input_json = json.dumps(input_data)\n",
    "        output_data = {\"Xtrain\" : \"\", \"ytrain\" : \"\", \"Xtest\" : \"\", \"ytest\" : \"\"}\n",
    "        output_json = json.dumps(output_data)\n",
    "        \n",
    "        self.istr_jsons = input_json\n",
    "        self.ostr_jsons = output_json\n",
    "        \n",
    "    def get_track_data(self, filepath):\n",
    "        h5 = hdf5_getters.open_h5_file_read(filepath)\n",
    "        keys = filter(lambda x: x[:3] == 'get',hdf5_getters.__dict__.keys())\n",
    "        track_data = {}\n",
    "        track_data['year'] = hdf5_getters.get_year(h5).item()\n",
    "        track_data['danceability'] = hdf5_getters.get_danceability(h5).item()\n",
    "        track_data['loudness'] = hdf5_getters.get_loudness(h5).item()\n",
    "        track_data['track_7digitalid'] = hdf5_getters.get_track_7digitalid(h5).item()\n",
    "        track_data['energy'] = hdf5_getters.get_energy(h5).item()\n",
    "        track_data['tempo'] = hdf5_getters.get_tempo(h5).item()\n",
    "        track_data['end_fade_in'] = hdf5_getters.get_end_of_fade_in(h5).item()\n",
    "        track_data['start_fade_out'] = hdf5_getters.get_start_of_fade_out(h5).item()\n",
    "        h5.close()\n",
    "        \n",
    "        return track_data\n",
    "            \n",
    "    def do_run(self, input_data, traversal):\n",
    "        if traversal == \"POST\":\n",
    "            if not \"Xtrain\" in input_data.keys():\n",
    "                input_data[\"Xtrain\"] = \"\"\n",
    "            if not \"ytrain\" in input_data.keys():\n",
    "                input_data[\"ytrain\"] = \"\"\n",
    "            if not \"Xtest\" in input_data.keys():\n",
    "                input_data[\"Xtest\"] = \"\"\n",
    "            if not \"ytest\" in input_data.keys():\n",
    "                input_data[\"ytest\"] = \"\"\n",
    "            return input_data\n",
    "            \n",
    "        self.h5_files = input_data['h5_files']\n",
    "        \n",
    "        self.nsongs = input_data['nsongs'] = len(self.h5_files)\n",
    "    \n",
    "        X, y = self.extract_X_y(self.get_track_data, input_data, self.h5_files)\n",
    "        \n",
    "        # y is returned as a 2D array to support multilabel prediction. In our case we only have 1 label per example\n",
    "        # so reshape into 1D array\n",
    "        y = y.reshape((y.shape[0]))\n",
    "        \n",
    "        # We are interested in classifyings into pre-2000 and post-2000 songs\n",
    "        dprint(DPRINT_DEBUG, \"Before transform: y.shape=\" + str(y.shape))\n",
    "        y = (y > 2000)\n",
    "        dprint(DPRINT_DEBUG, \"After transform: y.shape=\" + str(y.shape))\n",
    "        train_limit = int(self.train_frac * self.nsongs)\n",
    "        \n",
    "        # Let the parent add the useless \"features\" key\n",
    "        input_data[\"Xtrain\"], input_data[\"ytrain\"] = X[0:train_limit], y[0:train_limit]\n",
    "        input_data[\"Xtest\"], input_data[\"ytest\"] = X[train_limit:], y[train_limit:]\n",
    "        \n",
    "        dprint(DPRINT_INFO, \n",
    "            \"Xtrain.shape=\" + str(input_data[\"Xtrain\"].shape) + \", ytrain.shape=\" + str(input_data[\"ytrain\"].shape))\n",
    "        dprint(DPRINT_INFO, \n",
    "            \"Xtest.shape=\" + str(input_data[\"Xtest\"].shape) + \", ytest.shape=\" + str(input_data[\"ytest\"].shape))\n",
    "        \n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes on Million Song Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "feature_list = ['danceability', 'loudness', 'energy', 'tempo', 'end_fade_in', 'start_fade_out']\n",
    "label_list = ['year']\n",
    "myML = MLRoot()\n",
    "myML.mount(mount_spec = \"./millsong_GaussNB_mounts.json\")\n",
    "myML.print_tree()\n",
    "input_data = {}\n",
    "input_data[\"remote_loc\"] = \"\"\n",
    "myML.compile(json.dumps(input_data))\n",
    "input_data[\"remote_loc\"] = \"/MillionSong\"\n",
    "# Set maximum size of data we want to process: 100 GB\n",
    "myML.setprop(\"/root/fetch\", {\"max_size\" : 100}) \n",
    "# Maximum number of files to process: 300,000\n",
    "myML.setprop(\"/root/fetch/raw/collect_h5\", {\"max_process\" : 300000})\n",
    "# Maximum number of examples (training+test) to collect: 150,000\n",
    "myML.setprop(\"/root/fetch/raw/collect_h5\", {\"nsongs\" : 150000})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"train_frac\" : 0.7})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"X_map\" : feature_list})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"y_map\" : label_list})\n",
    "myML.setprop(\"/root/model/gauss_nb\", {\"nsamples_list\" : nsamples_list})\n",
    "myML.run(input_data)\n",
    "myML.umount()\n",
    "dprint(DPRINT_INFO, \"Total Time taken: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on Million Song Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "feature_list = ['danceability', 'loudness', 'energy', 'tempo', 'end_fade_in', 'start_fade_out']\n",
    "label_list = ['year']\n",
    "myML = MLRoot()\n",
    "myML.mount(mount_spec = \"./millsong_logistic_mounts.json\")\n",
    "myML.print_tree()\n",
    "input_data = {}\n",
    "input_data[\"remote_loc\"] = \"\"\n",
    "myML.compile(json.dumps(input_data))\n",
    "input_data[\"remote_loc\"] = \"/MillionSong\"\n",
    "# Set maximum size of data we want to process: 100 GB\n",
    "myML.setprop(\"/root/fetch\", {\"max_size\" : 100}) \n",
    "# Maximum number of files to process: 300,000\n",
    "myML.setprop(\"/root/fetch/raw/collect_h5\", {\"max_process\" : 300000})\n",
    "# Maximum number of examples (training+test) to collect: 150,000\n",
    "myML.setprop(\"/root/fetch/raw/collect_h5\", {\"nsongs\" : 150000})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"train_frac\" : 0.7})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"X_map\" : feature_list})\n",
    "myML.setprop(\"/root/derive/millsong_extract\", {\"y_map\" : label_list})\n",
    "myML.setprop(\"/root/model/logistic\", {\"nsamples_list\" : nsamples_list})\n",
    "myML.run(input_data)\n",
    "myML.umount()\n",
    "dprint(DPRINT_INFO, \"Total Time taken: \" + str(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
